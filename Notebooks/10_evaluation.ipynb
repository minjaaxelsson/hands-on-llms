{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you might be wondering is how we can evaluate the RAG process. Well, it's hard. There are a few possible techniques we can use. And here we will demonstrate a few here:\n",
    "\n",
    "- Perplexity\n",
    "\n",
    "- Semantic similarity\n",
    "\n",
    "- Faithfulness\n",
    "\n",
    "The core of these final two methods (and many methods that evaluate RAG systems) involves feeding the entire paper into an LLM and asking it to generate some questions and some answers based on the paper. We can then assess things like semantic similarity. We can also ask the model to evaluate whether the answer it gave can actually be inferred from the context given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import fitz\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from jinja2 import Environment, FileSystemLoader, select_autoescape\n",
    "from typing import Any\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities\n",
    "A language model returns a probability distribution over tokens in order to give us an idea of which token to predict next.\n",
    "\n",
    "It is usually more convenient to work with the _logarithm_ of these probabilities (logprobs) for a few theoretical and practical reasons:\n",
    "\n",
    "- It turns multiplications into additions, which is handy if you want to look at sequences of outputs.\n",
    "\n",
    "- It helps with numerical issues. Multiplying very small numbers could cause underflow in floating point operations. Taking the log converts small numbers to big numbers.\n",
    "\n",
    "But we can also get the logprobs from OpenAI models (and many other models) in order to develop metrics:\n",
    "\n",
    "- Classification tasks: a measure of confidence in the result;\n",
    "\n",
    "- During RAG: confidence of whether the answer is contained in the retrieved context;\n",
    "\n",
    "- Autocomplete;\n",
    "\n",
    "- Perplexity: overall confidence in a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    max_tokens=512,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=420,\n",
    "    tools=None,\n",
    "    logprobs=None,\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"You will be given a list of sentences to classify into a particular field of study. \"\n",
    "    \"You will need to classify each sentence into one of the following categories:\\n\"\n",
    "    \"- Physics\\n\"\n",
    "    \"- Biology\\n\"\n",
    "    \"- Computer Science\\n\"\n",
    "    \"Respond only with one of these categories.\\n\\n\"\n",
    "    \"Sentence: {sentence}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Connections between neurons can be mapped by acquiring and analysing electron microscopic brain images.\n",
      "Classification: Biology\n",
      "\n",
      "Sentence: A straightforward way to quantify the creation of light is through the coefficient of spontaneous emission.\n",
      "Classification: Physics\n",
      "\n",
      "Sentence: This method optimizes the simulation of protein folding using deep learning.\n",
      "Classification: Biology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Connections between neurons can be mapped by acquiring and analysing electron microscopic brain images.\",\n",
    "    \"A straightforward way to quantify the creation of light is through the coefficient of spontaneous emission.\",\n",
    "    \"This method optimizes the simulation of protein folding using deep learning.\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt.format(sentence=sentence)}]\n",
    "    completion = get_completion(messages, model=\"gpt-4o-mini\")\n",
    "\n",
    "    print(f\"Sentence: {sentence}\\nClassification: {completion.choices[0].message.content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can return the top token and the logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Connections between neurons can be mapped by acquiring and analysing electron microscopic brain images.\n",
      "Classification: Biology\n",
      "Logprobs: 100.00\n",
      "\n",
      "Sentence: A straightforward way to quantify the creation of light is through the coefficient of spontaneous emission.\n",
      "Classification: Physics\n",
      "Logprobs: 100.00\n",
      "\n",
      "Sentence: This method optimizes the simulation of protein folding using deep learning.\n",
      "Classification: Biology\n",
      "Logprobs: 99.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for sentence in sentences:\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt.format(sentence=sentence)}]\n",
    "    completion = get_completion(\n",
    "        messages,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=64,\n",
    "        logprobs=True,\n",
    "        top_logprobs=2,\n",
    "    )\n",
    "    logprobs = completion.choices[0].logprobs.content[0]\n",
    "\n",
    "    print(f\"Sentence: {sentence}\\n\"\n",
    "          f\"Classification: {completion.choices[0].message.content}\\n\"\n",
    "          f\"Logprobs: {math.exp(logprobs.logprob)*100:.2f}\\n\"\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "Perplexity can be considered a measure of uncertainty. In the context of LLMs, it is calculated by taking the average of the logprobs and exponentiating the negative. If we have a tokenized sequence $X = (x_0, x_1, ... x_t), then the perplexity is\n",
    "\n",
    "$$\n",
    "\\textrm{PPL}(X) = \\exp\\left\\{-\\frac{1}{t}\\sum_i^t \\log p_\\theta(x_i | x_{<i})\\right\\}\n",
    "$$\n",
    "\n",
    "The $\\log p_\\theta(x_i | x_{<i})$ term is the log-likelihood of the $i^{\\textrm{th}}$ token conditioned on the preceding tokens before $i$. Let's look at an example. To see this in action, we ask two questions: one that has a fairly certain answer, and another that is more speculative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"In a few sentences, consicely summarize the theory of special relativity.\",\n",
    "    \"In a few sentences, consicely explain who you think will win the 2025 Formula One Drivers' Championship.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In a few sentences, consicely summarize the theory of special relativity.\n",
      "Answer: The theory of special relativity, formulated by Albert Einstein in 1905, posits that the laws of physics are the same for all observers, regardless of their relative motion, and that the speed of light in a vacuum is constant for all observers. It introduces the concepts of time dilation and length contraction, which describe how time and space are perceived differently depending on the relative velocity of observers. Special relativity also leads to the famous equation E=mcÂ², establishing the equivalence of mass and energy.\n",
      "Perplexity: 1.14\n",
      "\n",
      "Question: In a few sentences, consicely explain who you think will win the 2025 Formula One Drivers' Championship.\n",
      "Answer: Predicting the winner of the 2025 Formula One Drivers' Championship is challenging, as it depends on various factors such as team performance, driver skill, and technological advancements. However, if current trends continue, drivers like Max Verstappen or Charles Leclerc, who have shown consistent performance and are part of strong teams, could be strong contenders. Ultimately, the outcome will hinge on the developments in the 2024 season and beyond.\n",
      "Perplexity: 1.26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for question in questions:\n",
    "    messages = [{\"role\": \"system\", \"content\": question}]\n",
    "    completion = get_completion(messages, model=\"gpt-4o-mini\", logprobs=True, temperature=0.0)\n",
    "\n",
    "    log_probs = [token.logprob for token in completion.choices[0].logprobs.content]\n",
    "    temperature = 1.7\n",
    "    response = completion.choices[0].message.content\n",
    "    perplexity_score = np.exp(-np.mean(log_probs))\n",
    "\n",
    "    print(\n",
    "        f\"Question: {question}\\nAnswer: {completion.choices[0].message.content}\\n\"\n",
    "        f\"Perplexity: {perplexity_score:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the more speculative answer, the perplexity is higher. Now try increasing the temperature for `0.0` to something like `0.7` and see what happens to the scores..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating RAG using synthetic data\n",
    "In these next examples, we look at some methods to evaluate RAG - semantic similarity and faithfulness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same approach as previous notebook. So we have moved a bunch of our code into a `utils.py` file. We have mostly kept things the same, but have a look over it and make sure you understand how it all works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import chunker, DocumentDB, load_template\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"data/paper.pdf\")\n",
    "text_chunks, doc_idxs = chunker(chunk_size=1024, overlap=128, documents=documents)\n",
    "\n",
    "doc_db = DocumentDB(\"paper_db\", path=\"../data-storage-and-ingestion/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate question answer pairs\n",
    "For this, we will use `gpt-4o` because we want high quality question answer pairs. Ideally, you would do this with humans - subject matter experts would carefully hand-craft these pairs.\n",
    "\n",
    "The first stage is to then generate 10 Q&A pairs using pydantic again. The implementations presented here closely follow the method used by the [RAGAS](https://docs.ragas.io/en/stable/getstarted/index.html#get-started) library.\n",
    "\n",
    "We implement a Pydantic BaseModel class that will house our list of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'questions': {'items': {'type': 'string'}, 'title': 'List of questions', 'type': 'array'}, 'answers': {'items': {'type': 'string'}, 'title': 'List of answers', 'type': 'array'}}, 'required': ['questions', 'answers'], 'title': 'QAPairs', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "class QAPairs(BaseModel):\n",
    "    questions: list[str] = Field(..., title=\"List of questions\")\n",
    "    answers: list[str] = Field(..., title=\"List of answers\")\n",
    "\n",
    "print(QAPairs.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a prompt that we can use to generate these Q&A pairs. It looks something like this:\n",
    "\n",
    "---\n",
    "```\n",
    "You are a reading comprehension system that is an expert at extracting information from academic papers.\n",
    "Your task is to carefully read the provided text \"CONTEXT\" and then generate question and answer pairs.\n",
    "Your questions should be concise. Your answers should be as detailed as possible, including any mathematical or numerical results from the text.\n",
    "You should aim to produce approximately one paragraph for your answers (100-200 words).\n",
    "Your questions should be a mixture of general, high-level concepts, and also highly detailed questions about specific points, including any mathematical or numerical results.\n",
    "You should respond in JSON format according to the following schema:\n",
    "\n",
    "{{ schema }}\n",
    "\n",
    "You should generate {{ number }} question and answer pairs.\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_qa = load_template(\n",
    "    \"prompts/evaluation/qa_generation_system_prompt.jinja\",\n",
    "    {\n",
    "        \"number\" : 10,\n",
    "        \"schema\" : QAPairs.model_json_schema()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a reading comprehension system that is an expert at extracting information from academic papers.\n",
      "Your task is to carefully read the provided text \"CONTEXT\" and then generate question and answer pairs.\n",
      "Your questions should be concise. Your answers should be as detailed as possible, including any mathematical or numerical results from the text.\n",
      "You should aim to produce approximately one paragraph for your answers (100-200 words).\n",
      "Your questions should be a mixture of general, high-level concepts, and also highly detailed questions about specific points, including any mathematical or numerical results.\n",
      "You should respond in JSON format according to the following schema:\n",
      "\n",
      "{'properties': {'questions': {'items': {'type': 'string'}, 'title': 'List of questions', 'type': 'array'}, 'answers': {'items': {'type': 'string'}, 'title': 'List of answers', 'type': 'array'}}, 'required': ['questions', 'answers'], 'title': 'QAPairs', 'type': 'object'}\n",
      "\n",
      "You should generate 10 question and answer pairs.\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to the pages of the pdf as a single text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = \" \".join([doc.text for doc in documents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are in a position to generate our question answer pairs using `gpt-4o`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "user_prompt = (\n",
    "    f\"CONTEXT:\\n\\n{pdf_text}\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt_qa},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the `QAPairs` object using the LLM output, and also save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52400/2352302158.py:5: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  json.dump(questions_answers.dict(), f, indent=4)\n"
     ]
    }
   ],
   "source": [
    "questions_answers = QAPairs(**json.loads(response.choices[0].message.content))\n",
    "\n",
    "# save the Q&A to file\n",
    "with open(\"data/qa.json\", \"w\") as f:\n",
    "    json.dump(questions_answers.dict(), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does an example look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the main achievements of large language models like GPT-4?\n",
      "---\n",
      "Large language models (LLMs) like GPT-4 have demonstrated remarkable proficiency in various language-based tasks, often surpassing human performance in areas such as essay writing, dialogue generation, and standardized testing. They have been reported to achieve scores in the 80-99th percentile on graduate admissions tests like the GRE and LSAT, and their programming abilities are said to compare favorably to those of average software engineers. Additionally, LLMs can solve complex mathematical problems and generate creative outputs, such as poetry, showcasing their versatility and advanced capabilities.\n"
     ]
    }
   ],
   "source": [
    "print(questions_answers.questions[0])\n",
    "print('---')\n",
    "print(questions_answers.answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try and do cosine similarity scores between the returned contexts and the actual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unsure about the main achievements of large language models like GPT-4, as there is no relevant information provided in the context.\n"
     ]
    }
   ],
   "source": [
    "from utils import rag_query\n",
    "\n",
    "example_query = questions_answers.questions[0]\n",
    "\n",
    "response, context = rag_query(\n",
    "    query=example_query,\n",
    "    n_context=5,\n",
    "    doc_db=doc_db,\n",
    "    return_context=True\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look at semantic similarity between the predicted response and the desired response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response_embedding = client.embeddings.create(\n",
    "    input=response,\n",
    "    model=\"text-embedding-3-small\"\n",
    ").data[0].embedding\n",
    "\n",
    "answer_embedding = client.embeddings.create(\n",
    "    input=questions_answers.answers[0],\n",
    "    model=\"text-embedding-3-small\"\n",
    ").data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56068731]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([response_embedding], [answer_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unsure about the main achievements of large language models like GPT-4, as there is no relevant information provided in the context.\n",
      "---\n",
      "Large language models (LLMs) like GPT-4 have demonstrated remarkable proficiency in various language-based tasks, often surpassing human performance in areas such as essay writing, dialogue generation, and standardized testing. They have been reported to achieve scores in the 80-99th percentile on graduate admissions tests like the GRE and LSAT, and their programming abilities are said to compare favorably to those of average software engineers. Additionally, LLMs can solve complex mathematical problems and generate creative outputs, such as poetry, showcasing their versatility and advanced capabilities.\n"
     ]
    }
   ],
   "source": [
    "print(response)\n",
    "print('---')\n",
    "print(questions_answers.answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well OK, but what does this score mean? It is simply a measure of the similarity of the embeddings. It gives no real indication if the output is \"better\" or \"worse\" or more or less informative than the original answer. It is important to consider these scores in context of your overall objective. It is also important to curate good quality Q&A pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness\n",
    "This is a little more complicated. First, we get an LLM to extract key statements from the answer. For example:\n",
    "\n",
    "```python\n",
    "[\n",
    "    ['This study was conducted by Mallinson et al.'],\n",
    "    ['The main focus is to investigate avalanches and criticality in self-organized nanoscale network.']\n",
    "    ['They analyzed electrical conductance.']\n",
    "    ['They analyzed the behavior of the networks under various stimulus conditions.']\n",
    "]\n",
    "```\n",
    "\n",
    "We then ask a second LLM to look at each statement and see if that statement can be inferred from the text, assigning a score of 0 for no, and 1 for yes.\n",
    "\n",
    "To do this, we create two additional Pydantic classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Statements(BaseModel):\n",
    "    simpler_statements: list[str] = Field(..., description=\"the simpler statements\")\n",
    "\n",
    "\n",
    "class StatementFaithfulnessAnswer(BaseModel):\n",
    "    statement: str = Field(..., description=\"the original statement, word-for-word\")\n",
    "    reason: str = Field(..., description=\"the reason of the verdict\")\n",
    "    verdict: int = Field(..., description=\"the verdict(0/1) of the faithfulness.\")\n",
    "\n",
    "\n",
    "class Faithfulness(BaseModel):\n",
    "    answers: list[StatementFaithfulnessAnswer] = Field(..., description=\"the faithfulness answers\")\n",
    "    score: float = Field(..., description=\"the average faithfulness score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create two more prompts `statement_instruction`, and `faithfulness_instruction`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```\n",
    "Given a piece of text, analyze the complexity of each sentence and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON, according to the following schema:\n",
    "\n",
    "{{ schema }}\n",
    "\n",
    "Here is a new piece of text:\n",
    "\n",
    "{{ statement }}\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```\n",
    "Your task is to judge the faithfulness of a statement based on a given context. For the statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n",
    "\n",
    "You will give the exact statement, the reason, and the verdict.\n",
    "\n",
    "Format the outputs in JSON, according to the following schema:\n",
    "\n",
    "{{ schema }}\n",
    "\n",
    "Here is a statement:\n",
    "\n",
    "{{ statement }}\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements(answer):\n",
    "    prompt = load_template(\n",
    "        \"prompts/faithfulness/statement_instruction.jinja\",\n",
    "        {\n",
    "            \"schema\" : Statements.model_json_schema(),\n",
    "            \"text\" : answer\n",
    "        }\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": answer}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        logprobs=True,\n",
    "    )\n",
    "\n",
    "    return Statements(**json.loads(completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = get_statements(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unsure about the main achievements of large language models like GPT-4, as there is no relevant information provided in the context.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Statements</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   </span><span style=\"color: #808000; text-decoration-color: #808000\">simpler_statements</span>=<span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   </span><span style=\"color: #008000; text-decoration-color: #008000\">'There is uncertainty regarding the main achievements of large language models like GPT-4.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   </span><span style=\"color: #008000; text-decoration-color: #008000\">'No relevant information is provided in the context.'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   </span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mStatements\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32mâ   \u001b[0m\u001b[33msimpler_statements\u001b[0m=\u001b[1m[\u001b[0m\n",
       "\u001b[2;32mâ   â   \u001b[0m\u001b[32m'There is uncertainty regarding the main achievements of large language models like GPT-4.'\u001b[0m,\n",
       "\u001b[2;32mâ   â   \u001b[0m\u001b[32m'No relevant information is provided in the context.'\u001b[0m\n",
       "\u001b[2;32mâ   \u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.pretty import pprint\n",
    "print(response)\n",
    "pprint(statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithfulness(statements : Statements, context):\n",
    "    context_joined = \" \".join(context)\n",
    "    faithfulness_answers = []\n",
    "\n",
    "    for statement in statements.simpler_statements:\n",
    "        prompt = load_template(\n",
    "            \"prompts/faithfulness/faithfulness_instruction.jinja\",\n",
    "            {\n",
    "                \"schema\" : StatementFaithfulnessAnswer.model_json_schema(),\n",
    "                \"statement\" : statement,\n",
    "                \"context\" : context_joined\n",
    "            }\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": context_joined}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        faithfulness_answers.append(StatementFaithfulnessAnswer(**json.loads(response)))\n",
    "\n",
    "    score = sum([answer.verdict for answer in faithfulness_answers]) / len(faithfulness_answers)\n",
    "\n",
    "    return Faithfulness(answers=faithfulness_answers, score=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_faithfulness(statements, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Faithfulness</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   </span><span style=\"color: #808000; text-decoration-color: #808000\">answers</span>=<span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   â   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'There is uncertainty regarding the main achievements of large language models like GPT-4.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   â   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The context does not provide any information about the achievements or uncertainties related to large language models like GPT-4. It only mentions the training data cutoff date, which does not imply any uncertainty about achievements.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   â   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   â   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'No relevant information is provided in the context.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   â   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The context states that the training data is up to October 2023, which implies that there is relevant information regarding the timeframe of the data. Therefore, the statement is not accurate.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   â   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   â   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   </span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">â   </span><span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mFaithfulness\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32mâ   \u001b[0m\u001b[33manswers\u001b[0m=\u001b[1m[\u001b[0m\n",
       "\u001b[2;32mâ   â   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32mâ   â   â   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'There is uncertainty regarding the main achievements of large language models like GPT-4.'\u001b[0m,\n",
       "\u001b[2;32mâ   â   â   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m'The context does not provide any information about the achievements or uncertainties related to large language models like GPT-4. It only mentions the training data cutoff date, which does not imply any uncertainty about achievements.'\u001b[0m,\n",
       "\u001b[2;32mâ   â   â   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m0\u001b[0m\n",
       "\u001b[2;32mâ   â   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32mâ   â   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32mâ   â   â   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'No relevant information is provided in the context.'\u001b[0m,\n",
       "\u001b[2;32mâ   â   â   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m'The context states that the training data is up to October 2023, which implies that there is relevant information regarding the timeframe of the data. Therefore, the statement is not accurate.'\u001b[0m,\n",
       "\u001b[2;32mâ   â   â   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m0\u001b[0m\n",
       "\u001b[2;32mâ   â   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32mâ   \u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32mâ   \u001b[0m\u001b[33mscore\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
